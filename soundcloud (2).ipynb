{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All log_times are constant for 'Garme'. Skipping gradient calculation.\n",
      "Warning: All log_times are constant for 'KARBALA'. Skipping gradient calculation.\n",
      "Unusual changes for 'Be Qiafat Nemiad':\n",
      "  Log Time: 1721869016225, Count: 819204, Gradient: 0.00030702546388641\n",
      "Unusual changes for 'Butterfly':\n",
      "  Log Time: 1718765817007, Count: 68371, Gradient: 0.0004962094906690727\n",
      "Unusual changes for 'Ghatle Amd':\n",
      "  Log Time: 1721523416225, Count: 1337229, Gradient: 0.001133206013483594\n",
      "  Log Time: 1721609816225, Count: 1521678, Gradient: 0.0011301273148148147\n",
      "Unusual changes for 'HICHKI BE JOZ TO II':\n",
      "  Log Time: 1718328998630, Count: 576598, Gradient: 3.907407407139534e-05\n",
      "Unusual changes for 'Hasta La Vista':\n",
      "  Log Time: 1718765817007, Count: 103489, Gradient: 0.0006153761601033363\n",
      "Unusual changes for 'Khaterate Kohan':\n",
      "  Log Time: 1717967251465, Count: 2630, Gradient: 3.235221674594349e-07\n",
      "Unusual changes for 'Nagi Jayi':\n",
      "  Log Time: 1717967251465, Count: 28035, Gradient: 1.138926211757818e-06\n",
      "Unusual changes for 'Tehroon':\n",
      "  Log Time: 1719171785062, Count: 1185979, Gradient: 0.00010142713388968695\n",
      "Unusual changes for 'Ye Jaye Door':\n",
      "  Log Time: 1717353015390, Count: 104885, Gradient: 1.0578703703703704e-05\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open('soundcloudLogs.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to group logs by track name and collect counts over time\n",
    "def group_logs_and_collect_counts(data):\n",
    "    grouped_logs = {}\n",
    "    for item in data:\n",
    "        name = item.get('name', 'Unknown')\n",
    "        log = item.get('soundcloudLog', {})\n",
    "        if name not in grouped_logs:\n",
    "            grouped_logs[name] = []\n",
    "        grouped_logs[name].append(log)\n",
    "    \n",
    "    # Sort logs by logTime and extract counts\n",
    "    counts_by_name = {}\n",
    "    for name, logs in grouped_logs.items():\n",
    "        logs.sort(key=lambda x: x['logTime'])\n",
    "        counts_by_name[name] = [(log['logTime'], log['counts']) for log in logs]\n",
    "    \n",
    "    return counts_by_name\n",
    "\n",
    "# Function to remove records where count on day n+1 is smaller than on day n\n",
    "def remove_invalid_records(counts_by_name):\n",
    "    cleaned_counts_by_name = {}\n",
    "    for name, counts in counts_by_name.items():\n",
    "        cleaned_counts = []\n",
    "        for i in range(len(counts) - 1):\n",
    "            current_log_time, current_count = counts[i]\n",
    "            next_log_time, next_count = counts[i + 1]\n",
    "            # Keep the record if the next day's count is greater than or equal to the current day's count\n",
    "            if next_count >= current_count:\n",
    "                cleaned_counts.append((current_log_time, current_count))\n",
    "        # Append the last record\n",
    "        if counts:\n",
    "            cleaned_counts.append(counts[-1])\n",
    "        cleaned_counts_by_name[name] = cleaned_counts\n",
    "    \n",
    "    return cleaned_counts_by_name\n",
    "\n",
    "# Group logs and collect counts\n",
    "counts_by_name = group_logs_and_collect_counts(data)\n",
    "\n",
    "# Clean the data by removing invalid records\n",
    "cleaned_counts_by_name = remove_invalid_records(counts_by_name)\n",
    "\n",
    "# Function to calculate gradient and report unusual changes\n",
    "def analyze_gradients(counts_by_name):\n",
    "    unusual_changes = {}\n",
    "    for name, counts in counts_by_name.items():\n",
    "        log_times = np.array([log_time for log_time, _ in counts])\n",
    "        log_counts = np.array([count for _, count in counts])\n",
    "        \n",
    "        # Check if log_times has constant values\n",
    "        if np.all(np.diff(log_times) == 0):\n",
    "            print(f\"Warning: All log_times are constant for '{name}'. Skipping gradient calculation.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        try:\n",
    "            gradients = np.gradient(log_counts, log_times)\n",
    "        except ZeroDivisionError:\n",
    "            print(f\"Error: Division by zero encountered while calculating gradients for '{name}'.\")\n",
    "            continue\n",
    "        \n",
    "        # Identify unusual changes\n",
    "        avg_gradient = np.mean(gradients)\n",
    "        std_gradient = np.std(gradients)\n",
    "        threshold = avg_gradient + 5 * std_gradient\n",
    "        \n",
    "        unusual_points = []\n",
    "        for i, gradient in enumerate(gradients):\n",
    "            if gradient > threshold:\n",
    "                unusual_points.append((log_times[i], log_counts[i], gradient))\n",
    "        \n",
    "        if unusual_points:\n",
    "            unusual_changes[name] = unusual_points\n",
    "    \n",
    "    return unusual_changes\n",
    "\n",
    "\n",
    "# Analyze gradients and report unusual changes on cleaned data\n",
    "unusual_changes = analyze_gradients(cleaned_counts_by_name)\n",
    "\n",
    "# Print unusual changes\n",
    "for name, changes in unusual_changes.items():\n",
    "    print(f\"Unusual changes for '{name}':\")\n",
    "    for log_time, count, gradient in changes:\n",
    "        print(f\"  Log Time: {log_time}, Count: {count}, Gradient: {gradient}\")\n",
    "\n",
    "# The cleaned data is stored in cleaned_counts_by_name and can be used for further analysis or saving to a new JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not enough data or constant log_times for 'Garme'. Skipping gradient calculation.\n",
      "Warning: Not enough data or constant log_times for 'KARBALA'. Skipping gradient calculation.\n",
      "Track 'Be Qiafat Nemiad' has a fake stream probability of 0.02\n",
      "Track 'Butterfly' has a fake stream probability of 0.02\n",
      "Track 'Ghatle Amd' has a fake stream probability of 0.03\n",
      "Track 'HICHKI BE JOZ TO II' has a fake stream probability of 0.02\n",
      "Track 'Hasta La Vista' has a fake stream probability of 0.02\n",
      "Track 'Khaterate Kohan' has a fake stream probability of 0.02\n",
      "Track 'Nagi Jayi' has a fake stream probability of 0.02\n",
      "Track 'Tehroon' has a fake stream probability of 0.02\n",
      "Track 'Ye Jaye Door' has a fake stream probability of 0.02\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open('soundcloudLogs.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to group logs by track name and collect counts over time\n",
    "def group_logs_and_collect_counts(data):\n",
    "    grouped_logs = {}\n",
    "    for item in data:\n",
    "        name = item.get('name', 'Unknown')\n",
    "        log = item.get('soundcloudLog', {})\n",
    "        if name not in grouped_logs:\n",
    "            grouped_logs[name] = []\n",
    "        grouped_logs[name].append(log)\n",
    "    \n",
    "    # Sort logs by logTime and extract counts\n",
    "    counts_by_name = {}\n",
    "    for name, logs in grouped_logs.items():\n",
    "        logs.sort(key=lambda x: x['logTime'])\n",
    "        counts_by_name[name] = [(log['logTime'], log['counts']) for log in logs]\n",
    "    \n",
    "    return counts_by_name\n",
    "\n",
    "# Function to remove records where count on day n+1 is smaller than on day n\n",
    "def remove_invalid_records(counts_by_name):\n",
    "    cleaned_counts_by_name = {}\n",
    "    for name, counts in counts_by_name.items():\n",
    "        cleaned_counts = []\n",
    "        for i in range(len(counts) - 1):\n",
    "            current_log_time, current_count = counts[i]\n",
    "            next_log_time, next_count = counts[i + 1]\n",
    "            # Keep the record if the next day's count is greater than or equal to the current day's count\n",
    "            if next_count >= current_count:\n",
    "                cleaned_counts.append((current_log_time, current_count))\n",
    "        # Append the last record\n",
    "        if counts:\n",
    "            cleaned_counts.append(counts[-1])\n",
    "        cleaned_counts_by_name[name] = cleaned_counts\n",
    "    \n",
    "    return cleaned_counts_by_name\n",
    "\n",
    "# Function to calculate gradient and report unusual changes\n",
    "def analyze_gradients(counts_by_name):\n",
    "    unusual_changes = {}\n",
    "    track_probabilities = {}\n",
    "\n",
    "    for name, counts in counts_by_name.items():\n",
    "        log_times = np.array([log_time for log_time, _ in counts])\n",
    "        log_counts = np.array([count for _, count in counts])\n",
    "        \n",
    "        # Check if log_times has constant values\n",
    "        if len(log_times) < 2 or np.all(np.diff(log_times) == 0):\n",
    "            print(f\"Warning: Not enough data or constant log_times for '{name}'. Skipping gradient calculation.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        try:\n",
    "            gradients = np.gradient(log_counts, log_times)\n",
    "        except ZeroDivisionError:\n",
    "            print(f\"Error: Division by zero encountered while calculating gradients for '{name}'.\")\n",
    "            continue\n",
    "        \n",
    "        # Identify unusual changes\n",
    "        avg_gradient = np.mean(gradients)\n",
    "        std_gradient = np.std(gradients)\n",
    "        threshold = avg_gradient + 5 * std_gradient\n",
    "        \n",
    "        unusual_points = []\n",
    "        for i, gradient in enumerate(gradients):\n",
    "            if gradient > threshold:\n",
    "                unusual_points.append((log_times[i], log_counts[i], gradient))\n",
    "        \n",
    "        if unusual_points:\n",
    "            unusual_changes[name] = unusual_points\n",
    "            # Calculate the probability of unusual points\n",
    "            track_probability = len(unusual_points) / len(counts) if len(counts) > 0 else 0\n",
    "            track_probabilities[name] = track_probability\n",
    "    \n",
    "    return unusual_changes, track_probabilities\n",
    "\n",
    "# Function to plot stream counts and highlight unusual points for tracks with unusual changes\n",
    "def plot_stream_counts_with_unusual_points(cleaned_counts_by_name, unusual_changes):\n",
    "    for idx, (name, unusual_points) in enumerate(unusual_changes.items(), start=1):\n",
    "        counts = cleaned_counts_by_name[name]\n",
    "        log_times = [log_time for log_time, _ in counts]\n",
    "        log_counts = [count for _, count in counts]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(log_times, log_counts, label='Stream Counts', color='blue')\n",
    "        \n",
    "        # Highlight unusual points\n",
    "        unusual_times = [time for time, _, _ in unusual_points]\n",
    "        unusual_counts = [count for _, count, _ in unusual_points]\n",
    "        plt.scatter(unusual_times, unusual_counts, color='red', label='Unusual Points')\n",
    "        \n",
    "        plt.title(f'Stream Counts for {name}')\n",
    "        plt.xlabel('Log Time')\n",
    "        plt.ylabel('Stream Count')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save the plot as a JPG file with a unique name\n",
    "        filename = f'soundcloud{idx}.jpg'\n",
    "        plt.savefig(filename, format='jpg')\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "# Group logs and collect counts\n",
    "counts_by_name = group_logs_and_collect_counts(data)\n",
    "\n",
    "# Clean the data by removing invalid records\n",
    "cleaned_counts_by_name = remove_invalid_records(counts_by_name)\n",
    "\n",
    "# Analyze gradients, report unusual changes, and calculate probability for each track with unusual changes\n",
    "unusual_changes, track_probabilities = analyze_gradients(cleaned_counts_by_name)\n",
    "\n",
    "# Print the probability of fake streams for each track with unusual changes\n",
    "for name, probability in track_probabilities.items():\n",
    "    print(f\"Track '{name}' has a fake stream probability of {probability:.2f}\")\n",
    "\n",
    "# Plot stream counts and highlight unusual points only for tracks with unusual changes\n",
    "plot_stream_counts_with_unusual_points(cleaned_counts_by_name, unusual_changes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
