{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 'Back Door' has a fake stream probability of 0.02\n",
      "Track 'Capital T' has a fake stream probability of 0.02\n",
      "Track 'Deal' has a fake stream probability of 0.01\n",
      "Track 'Gom o Goor' has a fake stream probability of 0.01\n",
      "Track 'Hichvaght' has a fake stream probability of 0.01\n",
      "Track 'LOVINOMORE' has a fake stream probability of 0.02\n",
      "Track 'Maria' has a fake stream probability of 0.02\n",
      "Track 'Merch' has a fake stream probability of 0.02\n",
      "Track 'Nutella' has a fake stream probability of 0.01\n",
      "Track 'Run' has a fake stream probability of 0.01\n",
      "Track 'Tamoome' has a fake stream probability of 0.02\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open('youtubeLogs.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to group logs by track name and collect counts over time\n",
    "def group_logs_and_collect_counts(data):\n",
    "    grouped_logs = {}\n",
    "    for item in data:\n",
    "        name = item.get('name', 'Unknown')\n",
    "        log = item.get('youtubeLog', {})\n",
    "        if name not in grouped_logs:\n",
    "            grouped_logs[name] = []\n",
    "        grouped_logs[name].append(log)\n",
    "    \n",
    "    # Sort logs by logTime and extract counts\n",
    "    counts_by_name = {}\n",
    "    for name, logs in grouped_logs.items():\n",
    "        logs.sort(key=lambda x: x['logTime'])\n",
    "        counts_by_name[name] = [(log['logTime'], log['primaryCounts']) for log in logs]\n",
    "    \n",
    "    return counts_by_name\n",
    "\n",
    "# Function to remove records where count on day n+1 is smaller than on day n\n",
    "def remove_invalid_records(counts_by_name):\n",
    "    cleaned_counts_by_name = {}\n",
    "    for name, counts in counts_by_name.items():\n",
    "        cleaned_counts = []\n",
    "        for i in range(len(counts) - 1):\n",
    "            current_log_time, current_count = counts[i]\n",
    "            next_log_time, next_count = counts[i + 1]\n",
    "            # Keep the record if the next day's count is greater than or equal to the current day's count\n",
    "            if next_count >= current_count:\n",
    "                cleaned_counts.append((current_log_time, current_count))\n",
    "        # Append the last record\n",
    "        if counts:\n",
    "            cleaned_counts.append(counts[-1])\n",
    "        cleaned_counts_by_name[name] = cleaned_counts\n",
    "    \n",
    "    return cleaned_counts_by_name\n",
    "\n",
    "# Function to calculate gradient and report unusual changes\n",
    "def analyze_gradients(counts_by_name):\n",
    "    unusual_changes = {}\n",
    "    track_probabilities = {}\n",
    "\n",
    "    for name, counts in counts_by_name.items():\n",
    "        if len(counts) < 2:\n",
    "            # Not enough data to calculate gradients\n",
    "            continue\n",
    "        \n",
    "        log_times = np.array([log_time for log_time, _ in counts])\n",
    "        log_counts = np.array([count for _, count in counts])\n",
    "        \n",
    "        # Ensure log_times is sorted\n",
    "        if not np.all(np.diff(log_times) >= 0):\n",
    "            raise ValueError(f\"Log times for track '{name}' are not strictly increasing.\")\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        gradients = np.gradient(log_counts, log_times)\n",
    "\n",
    "        # Identify unusual changes\n",
    "        avg_gradient = np.mean(gradients)\n",
    "        std_gradient = np.std(gradients)\n",
    "        threshold = avg_gradient + 6 * std_gradient\n",
    "        \n",
    "        unusual_points = []\n",
    "        for i, gradient in enumerate(gradients):\n",
    "            if gradient > threshold:\n",
    "                unusual_points.append((log_times[i], log_counts[i], gradient))\n",
    "        \n",
    "        if unusual_points:\n",
    "            unusual_changes[name] = unusual_points\n",
    "            track_probability = len(unusual_points) / len(counts)\n",
    "            track_probabilities[name] = track_probability\n",
    "    \n",
    "    return unusual_changes, track_probabilities\n",
    "\n",
    "# Function to plot stream counts and highlight unusual points for tracks with unusual changes\n",
    "def plot_stream_counts_with_unusual_points(cleaned_counts_by_name, unusual_changes):\n",
    "    for idx, (name, unusual_points) in enumerate(unusual_changes.items(), start=1):\n",
    "        counts = cleaned_counts_by_name[name]\n",
    "        log_times = [log_time for log_time, _ in counts]\n",
    "        log_counts = [count for _, count in counts]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(log_times, log_counts, label='Stream Counts', color='blue')\n",
    "        \n",
    "        # Highlight unusual points\n",
    "        unusual_times = [time for time, _, _ in unusual_points]\n",
    "        unusual_counts = [count for _, count, _ in unusual_points]\n",
    "        plt.scatter(unusual_times, unusual_counts, color='red', label='Unusual Points')\n",
    "        \n",
    "        plt.title(f'Stream Counts for {name}')\n",
    "        plt.xlabel('Log Time')\n",
    "        plt.ylabel('Stream Count')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save the plot as a JPG file with a unique name\n",
    "        filename = f'youtube{idx}.jpg'\n",
    "        plt.savefig(filename, format='jpg')\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "# Group logs and collect counts\n",
    "counts_by_name = group_logs_and_collect_counts(data)\n",
    "\n",
    "# Clean the data by removing invalid records\n",
    "cleaned_counts_by_name = remove_invalid_records(counts_by_name)\n",
    "\n",
    "# Analyze gradients, report unusual changes, and calculate probability for each track with unusual changes\n",
    "unusual_changes, track_probabilities = analyze_gradients(cleaned_counts_by_name)\n",
    "\n",
    "# Print the probability of fake streams for each track with unusual changes\n",
    "for name, probability in track_probabilities.items():\n",
    "    print(f\"Track '{name}' has a fake stream probability of {probability:.2f}\")\n",
    "\n",
    "# Plot stream counts and highlight unusual points only for tracks with unusual changes\n",
    "plot_stream_counts_with_unusual_points(cleaned_counts_by_name, unusual_changes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
